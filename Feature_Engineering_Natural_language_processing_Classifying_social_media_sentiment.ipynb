{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrTYzv5S+eN+GaNCY8k+uK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewDavidRatnam/Feature-_Engineering_Bookcamp-/blob/main/Feature_Engineering_Natural_language_processing_Classifying_social_media_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "import time"
      ],
      "metadata": {
        "id": "fTr-wCsh5j4j"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def advanced_grid_search(x_train, y_train, x_test, y_test, ml_pipeline, params, cv=3, include_probas=False, is_regression=False):\n",
        "  model_grid_search = GridSearchCV(ml_pipeline, param_grid=params, cv=cv, error_score=-1)\n",
        "  start_time = time.time()\n",
        "\n",
        "  model_grid_search.fit(x_train,y_train)\n",
        "\n",
        "  train_time = time.time()\n",
        "\n",
        "  print(f\"Training the model{(train_time - start_time):.2f} seconds\")\n",
        "\n",
        "  best_model = model_grid_search.best_estimator_\n",
        "\n",
        "  y_preds = best_model.predict(x_test)\n",
        "\n",
        "  if is_regression:\n",
        "    rmse = np.sqrt(mean_squared_error(y_pred=y_preds, y_true=y_test))\n",
        "    print(f'RMSE:{rmse:.5f}')\n",
        "  else:\n",
        "    print(classification_report(y_true=y_test, y_pred=y_preds))\n",
        "  print(f'Best params : {model_grid_search.best_params_}')\n",
        "\n",
        "  end_time = time.time()\n",
        "  print(f\"Overall took{(end_time - start_time):.2f} seconds\")\n",
        "\n",
        "  if include_probas:\n",
        "    y_probas = best_model.predict_proba(x_test).max(axis=1)\n",
        "    return best_model, y_preds, y_probas\n",
        "\n",
        "  return best_model, y_preds\n",
        "\n"
      ],
      "metadata": {
        "id": "NYirPKrM485O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ydata-profiling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhhmROtkNaQK",
        "outputId": "54100a55-a2fb-4b62-a7bf-edfcd818a92a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ydata-profiling in /usr/local/lib/python3.12/dist-packages (4.18.1)\n",
            "Requirement already satisfied: scipy<1.17,>=1.8 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (1.16.3)\n",
            "Requirement already satisfied: pandas!=1.4.0,<3.0,>1.5 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (2.2.2)\n",
            "Requirement already satisfied: matplotlib<=3.10,>=3.5 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (3.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (2.12.3)\n",
            "Requirement already satisfied: PyYAML<6.1,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (6.0.3)\n",
            "Requirement already satisfied: jinja2<3.2,>=3.1.6 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (3.1.6)\n",
            "Requirement already satisfied: visions<0.8.2,>=0.7.5 in /usr/local/lib/python3.12/dist-packages (from visions[type_image_path]<0.8.2,>=0.7.5->ydata-profiling) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.4,>=1.22 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (2.0.2)\n",
            "Requirement already satisfied: minify-html>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (0.18.1)\n",
            "Requirement already satisfied: filetype>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (1.2.0)\n",
            "Requirement already satisfied: phik<0.13,>=0.12.5 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (0.12.5)\n",
            "Requirement already satisfied: requests<3,>=2.32.0 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (2.32.4)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (4.67.3)\n",
            "Requirement already satisfied: seaborn<0.14,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (0.13.2)\n",
            "Requirement already satisfied: multimethod<2,>=1.4 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (1.12)\n",
            "Requirement already satisfied: statsmodels<1,>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (0.14.6)\n",
            "Requirement already satisfied: typeguard<5,>=4 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (4.4.4)\n",
            "Requirement already satisfied: imagehash==4.3.2 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (4.3.2)\n",
            "Requirement already satisfied: wordcloud>=1.9.4 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (1.9.6)\n",
            "Requirement already satisfied: dacite<2,>=1.9 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (1.9.2)\n",
            "Requirement already satisfied: numba<0.63,>=0.60 in /usr/local/lib/python3.12/dist-packages (from ydata-profiling) (0.60.0)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.12/dist-packages (from imagehash==4.3.2->ydata-profiling) (1.9.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from imagehash==4.3.2->ydata-profiling) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<3.2,>=3.1.6->ydata-profiling) (3.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<=3.10,>=3.5->ydata-profiling) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<=3.10,>=3.5->ydata-profiling) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<=3.10,>=3.5->ydata-profiling) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<=3.10,>=3.5->ydata-profiling) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<=3.10,>=3.5->ydata-profiling) (26.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<=3.10,>=3.5->ydata-profiling) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib<=3.10,>=3.5->ydata-profiling) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba<0.63,>=0.60->ydata-profiling) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=1.4.0,<3.0,>1.5->ydata-profiling) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=1.4.0,<3.0,>1.5->ydata-profiling) (2025.3)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.12/dist-packages (from phik<0.13,>=0.12.5->ydata-profiling) (1.5.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->ydata-profiling) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->ydata-profiling) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->ydata-profiling) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->ydata-profiling) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.0->ydata-profiling) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.0->ydata-profiling) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.0->ydata-profiling) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.0->ydata-profiling) (2026.1.4)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels<1,>=0.13.2->ydata-profiling) (1.0.2)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.12/dist-packages (from visions<0.8.2,>=0.7.5->visions[type_image_path]<0.8.2,>=0.7.5->ydata-profiling) (25.4.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.12/dist-packages (from visions<0.8.2,>=0.7.5->visions[type_image_path]<0.8.2,>=0.7.5->ydata-profiling) (3.6.1)\n",
            "Requirement already satisfied: puremagic in /usr/local/lib/python3.12/dist-packages (from visions<0.8.2,>=0.7.5->visions[type_image_path]<0.8.2,>=0.7.5->ydata-profiling) (1.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib<=3.10,>=3.5->ydata-profiling) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "16OxO5WaB2i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df = pd.read_csv('/content/cleaned_airline_tweets.csv')\n",
        "tweet_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "collapsed": true,
        "id": "VP3_-U0U5NVi",
        "outputId": "3892dcf4-40cf-401a-83ea-2790f87ae9a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text sentiment\n",
              "0                @VirginAmerica What @dhepburn said.   neutral\n",
              "1  @VirginAmerica it was amazing, and arrived an ...  positive\n",
              "2  @VirginAmerica I &lt;3 pretty graphics. so muc...  positive\n",
              "3  @VirginAmerica So excited for my first cross c...  positive\n",
              "4                    I ‚ù§Ô∏è flying @VirginAmerica. ‚ò∫Ô∏èüëç  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11a61379-fe9e-40a1-adfa-4e4ee2d38bf9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@VirginAmerica I &amp;lt;3 pretty graphics. so muc...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@VirginAmerica So excited for my first cross c...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I ‚ù§Ô∏è flying @VirginAmerica. ‚ò∫Ô∏èüëç</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11a61379-fe9e-40a1-adfa-4e4ee2d38bf9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-11a61379-fe9e-40a1-adfa-4e4ee2d38bf9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-11a61379-fe9e-40a1-adfa-4e4ee2d38bf9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tweet_df",
              "summary": "{\n  \"name\": \"tweet_df\",\n  \"rows\": 3860,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3860,\n        \"samples\": [\n          \"@JetBlue Submitted, hoping for quick decision, tied to another donation we just received:\\nProposal- 27th Annual Gala\\nOrg- Boundless Readers\",\n          \"@SouthwestAir think you have great people working for you.\",\n          \"@USAirways trying to book award travel leaving on red-eye Sunday night after midnight. Should I book the date in Sunday or Monday?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"neutral\",\n          \"positive\",\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3Cq4iCWMlrY",
        "outputId": "bb976507-ad9f-465f-e731-d3a960a65f8f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['text', 'sentiment'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df[\"sentiment\"].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUgiqZr9NC8P",
        "outputId": "2a86c24b-6373-4396-a355-dd98ce333eef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['neutral', 'positive', 'negative'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df[\"text\"].nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrWgpLIZNIWW",
        "outputId": "dccc418c-547a-4c29-b519-d8e6c18e9486"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3860"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df['sentiment'].value_counts(normalize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "YaTahOyMNUin",
        "outputId": "200fb79c-14c5-4d19-8be0-b91426372a86"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment\n",
              "positive    0.348705\n",
              "neutral     0.336528\n",
              "negative    0.314767\n",
              "Name: proportion, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>proportion</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>0.348705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.336528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>0.314767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "can't run this and save in github"
      ],
      "metadata": {
        "id": "mpVCENVGbLxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from  ydata_profiling import ProfileReport\n",
        "# profile = ProfileReport(tweet_df, title=\"Tweets Report\", explorative=True)\n",
        "# profile"
      ],
      "metadata": {
        "id": "yLaXjyPaQNE-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting our data into training and testing sets"
      ],
      "metadata": {
        "id": "ZDeJ1QZWaq07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(\n",
        "    tweet_df , test_size=0.2, random_state=0,stratify=tweet_df[\"sentiment\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Y1rHzc_GRByr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Count of tweets in training set:{train.shape[0]:,}\")\n",
        "print(f'Count of tweets in testing set:{test.shape[0]:,}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL21oJxLqRLM",
        "outputId": "e9917b6d-b5bb-4ba1-9542-e6adc442e402"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of tweets in training set:3,088\n",
            "Count of tweets in testing set:772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Definition and Success definition"
      ],
      "metadata": {
        "id": "AQ2qTbF1q2Z3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is kind of a givem, given a tweet can we classify it into the right sentiment. Accuracy seems like the standard approach"
      ],
      "metadata": {
        "id": "PfSHOLUzq9fA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Vectorization"
      ],
      "metadata": {
        "id": "fCwS3pwqq0Zt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 main ways:\n",
        "- bag of words\n",
        "- count vectorization\n",
        "- tf-idf"
      ],
      "metadata": {
        "id": "-Zf4yb4WsfeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature construction: Bag of words"
      ],
      "metadata": {
        "id": "fcwusvYUrcMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CountVectorizer(ngram_range=(1, 3))`"
      ],
      "metadata": {
        "id": "UXKAIdYxs5dA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count vectorization"
      ],
      "metadata": {
        "id": "M_Uj41HHrdS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "single_word = cv.fit_transform(train[\"text\"])\n",
        "\n",
        "print(single_word.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2nSNg5QtNnC",
        "outputId": "7906a72e-5385-4610-b55a-ee406f247d44"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3088, 6018)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- scikit-learn‚Äôs CountVectorizer module converts text samples\n",
        "into vectors\n",
        "- transform the corpus to be a matrix of fixed-length vectors"
      ],
      "metadata": {
        "id": "gpOZyAf2x7rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(single_word.todense(), columns=cv.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "qavnuzpGte9o",
        "outputId": "524ea993-6fb1-4dd0-fd81-5d442e526ca2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      00  000  000114  000ft  00pm  0167560070877  02  0200  03  0400  ...  \\\n",
              "0      0    0       0      0     0              0   0     0   0     0  ...   \n",
              "1      0    0       0      0     0              0   0     0   0     0  ...   \n",
              "2      0    0       0      0     0              0   0     0   0     0  ...   \n",
              "3      0    0       0      0     0              0   0     0   0     0  ...   \n",
              "4      0    0       0      0     0              0   0     0   0     0  ...   \n",
              "...   ..  ...     ...    ...   ...            ...  ..   ...  ..   ...  ...   \n",
              "3083   0    0       0      0     0              0   0     0   0     0  ...   \n",
              "3084   0    0       0      0     0              0   0     0   0     0  ...   \n",
              "3085   0    0       0      0     0              0   0     0   0     0  ...   \n",
              "3086   0    0       0      0     0              0   0     0   0     0  ...   \n",
              "3087   0    0       0      0     0              0   0     0   0     0  ...   \n",
              "\n",
              "      zacks_com  zakkohane  zero  zf5wjgtxzt  zgoqoxjbqy  zj76  zone  \\\n",
              "0             0          0     0           0           0     0     0   \n",
              "1             0          0     0           0           0     0     0   \n",
              "2             0          0     0           0           0     0     0   \n",
              "3             0          0     0           0           0     0     0   \n",
              "4             0          0     0           0           0     0     0   \n",
              "...         ...        ...   ...         ...         ...   ...   ...   \n",
              "3083          0          0     0           0           0     0     0   \n",
              "3084          0          0     0           0           0     0     0   \n",
              "3085          0          0     0           0           0     0     0   \n",
              "3086          0          0     0           0           0     0     0   \n",
              "3087          0          0     0           0           0     0     0   \n",
              "\n",
              "      zsdgzydnde  zukes  zv2pt6trk9  \n",
              "0              0      0           0  \n",
              "1              0      0           0  \n",
              "2              0      0           0  \n",
              "3              0      0           0  \n",
              "4              0      0           0  \n",
              "...          ...    ...         ...  \n",
              "3083           0      0           0  \n",
              "3084           0      0           0  \n",
              "3085           0      0           0  \n",
              "3086           0      0           0  \n",
              "3087           0      0           0  \n",
              "\n",
              "[3088 rows x 6018 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47bee0d5-e30a-45a7-b1c7-dcf958b643bc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>00</th>\n",
              "      <th>000</th>\n",
              "      <th>000114</th>\n",
              "      <th>000ft</th>\n",
              "      <th>00pm</th>\n",
              "      <th>0167560070877</th>\n",
              "      <th>02</th>\n",
              "      <th>0200</th>\n",
              "      <th>03</th>\n",
              "      <th>0400</th>\n",
              "      <th>...</th>\n",
              "      <th>zacks_com</th>\n",
              "      <th>zakkohane</th>\n",
              "      <th>zero</th>\n",
              "      <th>zf5wjgtxzt</th>\n",
              "      <th>zgoqoxjbqy</th>\n",
              "      <th>zj76</th>\n",
              "      <th>zone</th>\n",
              "      <th>zsdgzydnde</th>\n",
              "      <th>zukes</th>\n",
              "      <th>zv2pt6trk9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3083</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3084</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3085</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3086</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3087</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3088 rows √ó 6018 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47bee0d5-e30a-45a7-b1c7-dcf958b643bc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-47bee0d5-e30a-45a7-b1c7-dcf958b643bc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-47bee0d5-e30a-45a7-b1c7-dcf958b643bc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vectorize our text with the 20 most common tokens"
      ],
      "metadata": {
        "id": "-4dTeBFXx4RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(max_features=20)\n",
        "limited_vocab = cv.fit_transform(train[\"text\"])\n",
        "pd.DataFrame(limited_vocab.toarray(), index = train[\"text\"], columns=cv.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "Bdr9nyI-uYqY",
        "outputId": "e02d16f4-8b6b-4c59-9e30-a7c90d214e5f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    americanair  and  flight  \\\n",
              "text                                                                           \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.             0    0       0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...            0    0       0   \n",
              "@SouthwestAir I would.                                        0    0       0   \n",
              "@USAirways trying to Cancelled Flight a flight ...            0    0       2   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...            1    0       0   \n",
              "...                                                         ...  ...     ...   \n",
              "‚Äú@JetBlue: Our fleet's on fleek. http://t.co/b5...            0    0       0   \n",
              "@united caught earlier flight to ORD. Gate chec...            0    1       2   \n",
              "@AmericanAir hi when will your next set of flig...            1    0       0   \n",
              "@SouthwestAir Finally! Integration w/ passbook ...            0    0       1   \n",
              "@JetBlue @cflanagian she's on to something                    0    0       0   \n",
              "\n",
              "                                                    for  in  is  it  jetblue  \\\n",
              "text                                                                           \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.     0   0   0   0        1   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...    0   0   0   0        1   \n",
              "@SouthwestAir I would.                                0   0   0   0        0   \n",
              "@USAirways trying to Cancelled Flight a flight ...    0   0   0   0        0   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...    0   0   0   0        0   \n",
              "...                                                 ...  ..  ..  ..      ...   \n",
              "‚Äú@JetBlue: Our fleet's on fleek. http://t.co/b5...    0   0   0   0        1   \n",
              "@united caught earlier flight to ORD. Gate chec...    0   1   0   1        0   \n",
              "@AmericanAir hi when will your next set of flig...    1   0   0   0        0   \n",
              "@SouthwestAir Finally! Integration w/ passbook ...    0   0   1   0        0   \n",
              "@JetBlue @cflanagian she's on to something            0   0   0   0        1   \n",
              "\n",
              "                                                    me  my  of  on  \\\n",
              "text                                                                 \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.    0   0   0   0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...   0   0   0   1   \n",
              "@SouthwestAir I would.                               0   0   0   0   \n",
              "@USAirways trying to Cancelled Flight a flight ...   0   0   0   1   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...   0   0   0   0   \n",
              "...                                                 ..  ..  ..  ..   \n",
              "‚Äú@JetBlue: Our fleet's on fleek. http://t.co/b5...   0   0   0   1   \n",
              "@united caught earlier flight to ORD. Gate chec...   0   0   0   0   \n",
              "@AmericanAir hi when will your next set of flig...   0   0   1   0   \n",
              "@SouthwestAir Finally! Integration w/ passbook ...   1   0   0   0   \n",
              "@JetBlue @cflanagian she's on to something           0   0   0   1   \n",
              "\n",
              "                                                    southwestair  thanks  the  \\\n",
              "text                                                                            \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.              0       0    0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...             0       0    0   \n",
              "@SouthwestAir I would.                                         1       0    0   \n",
              "@USAirways trying to Cancelled Flight a flight ...             0       0    0   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...             0       0    0   \n",
              "...                                                          ...     ...  ...   \n",
              "‚Äú@JetBlue: Our fleet's on fleek. http://t.co/b5...             0       0    0   \n",
              "@united caught earlier flight to ORD. Gate chec...             0       0    0   \n",
              "@AmericanAir hi when will your next set of flig...             0       0    0   \n",
              "@SouthwestAir Finally! Integration w/ passbook ...             1       0    0   \n",
              "@JetBlue @cflanagian she's on to something                     0       0    0   \n",
              "\n",
              "                                                    to  united  usairways  \\\n",
              "text                                                                        \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.    1       0          0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...   0       0          0   \n",
              "@SouthwestAir I would.                               0       0          0   \n",
              "@USAirways trying to Cancelled Flight a flight ...   1       0          1   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...   0       0          0   \n",
              "...                                                 ..     ...        ...   \n",
              "‚Äú@JetBlue: Our fleet's on fleek. http://t.co/b5...   0       0          0   \n",
              "@united caught earlier flight to ORD. Gate chec...   1       1          0   \n",
              "@AmericanAir hi when will your next set of flig...   0       0          0   \n",
              "@SouthwestAir Finally! Integration w/ passbook ...   0       0          0   \n",
              "@JetBlue @cflanagian she's on to something           1       0          0   \n",
              "\n",
              "                                                    you  your  \n",
              "text                                                           \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.     0     0  \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...    0     0  \n",
              "@SouthwestAir I would.                                0     0  \n",
              "@USAirways trying to Cancelled Flight a flight ...    0     0  \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...    1     0  \n",
              "...                                                 ...   ...  \n",
              "‚Äú@JetBlue: Our fleet's on fleek. http://t.co/b5...    0     0  \n",
              "@united caught earlier flight to ORD. Gate chec...    1     0  \n",
              "@AmericanAir hi when will your next set of flig...    0     1  \n",
              "@SouthwestAir Finally! Integration w/ passbook ...    1     0  \n",
              "@JetBlue @cflanagian she's on to something            0     0  \n",
              "\n",
              "[3088 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a1a6671d-68f0-4b4f-8950-b69a09c0ea18\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>americanair</th>\n",
              "      <th>and</th>\n",
              "      <th>flight</th>\n",
              "      <th>for</th>\n",
              "      <th>in</th>\n",
              "      <th>is</th>\n",
              "      <th>it</th>\n",
              "      <th>jetblue</th>\n",
              "      <th>me</th>\n",
              "      <th>my</th>\n",
              "      <th>of</th>\n",
              "      <th>on</th>\n",
              "      <th>southwestair</th>\n",
              "      <th>thanks</th>\n",
              "      <th>the</th>\n",
              "      <th>to</th>\n",
              "      <th>united</th>\n",
              "      <th>usairways</th>\n",
              "      <th>you</th>\n",
              "      <th>your</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>@JetBlue Maybe I'll just go to Cleveland instead.</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smh RT @JetBlue: Our fleet's on fleek. http://t.co/IRiXaIfJJX</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>@SouthwestAir I would.</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>@USAirways trying to Cancelled Flight a flight urgently...get hung up on twice??? Sweet refund policy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>@AmericanAir you are beyond redemption. Jfk. Baggage claim looks like a luggage warehouse</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>‚Äú@JetBlue: Our fleet's on fleek. http://t.co/b5ttno68xu‚Äù I just üôà</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>@united caught earlier flight to ORD. Gate checked bag, and you've lost it at O'Hare. original flight lands in 20minutes. #frustrating!</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>@AmericanAir hi when will your next set of flights be out for next year from Dublin???</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>@SouthwestAir Finally! Integration w/ passbook is a great Valentine gift - better then chocoLate Flight. You do heart me.</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>@JetBlue @cflanagian she's on to something</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3088 rows √ó 20 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1a6671d-68f0-4b4f-8950-b69a09c0ea18')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a1a6671d-68f0-4b4f-8950-b69a09c0ea18 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a1a6671d-68f0-4b4f-8950-b69a09c0ea18');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 3088,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3088,\n        \"samples\": [\n          \"@JetBlue Hello we are doing a world record attempt on the amount of ball point pens in a collection please could you help with a pen?\",\n          \"@SouthwestAir please do. Hate having to fly a different airline. You're my fav.\",\n          \"@SouthwestAir App Updated With #Passbook Support http://t.co/WrWqrUblPs via @ubergizmo\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"americanair\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"and\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"for\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"in\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"it\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"jetblue\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"me\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"my\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"of\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"on\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"southwestair\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thanks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"the\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"to\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1,\n          0,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"united\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"usairways\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"you\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"your\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(ngram_range=(1,3))\n",
        "more_ngrams = cv.fit_transform(train[\"text\"])\n",
        "print(more_ngrams.shape)\n",
        "pd.DataFrame(more_ngrams.toarray(), index = train['text'], columns = cv.get_feature_names_out()).head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "28ltdXZNuYoB",
        "outputId": "1fd7da00-6b56-4576-eb95-dd1d72c61c01"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3088, 70613)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    00  00 phone  \\\n",
              "text                                                               \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.    0         0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...   0         0   \n",
              "@SouthwestAir I would.                               0         0   \n",
              "@USAirways trying to Cancelled Flight a flight ...   0         0   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...   0         0   \n",
              "\n",
              "                                                    00 phone hold  00 pm  \\\n",
              "text                                                                       \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.               0      0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...              0      0   \n",
              "@SouthwestAir I would.                                          0      0   \n",
              "@USAirways trying to Cancelled Flight a flight ...              0      0   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...              0      0   \n",
              "\n",
              "                                                    00 pm that  000  000 air  \\\n",
              "text                                                                           \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.            0    0        0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...           0    0        0   \n",
              "@SouthwestAir I would.                                       0    0        0   \n",
              "@USAirways trying to Cancelled Flight a flight ...           0    0        0   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...           0    0        0   \n",
              "\n",
              "                                                    000 air miles  \\\n",
              "text                                                                \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.               0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...              0   \n",
              "@SouthwestAir I would.                                          0   \n",
              "@USAirways trying to Cancelled Flight a flight ...              0   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...              0   \n",
              "\n",
              "                                                    000 crewmembers  \\\n",
              "text                                                                  \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.                 0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...                0   \n",
              "@SouthwestAir I would.                                            0   \n",
              "@USAirways trying to Cancelled Flight a flight ...                0   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...                0   \n",
              "\n",
              "                                                    000 crewmembers embody  \\\n",
              "text                                                                         \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.                        0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...                       0   \n",
              "@SouthwestAir I would.                                                   0   \n",
              "@USAirways trying to Cancelled Flight a flight ...                       0   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...                       0   \n",
              "\n",
              "                                                    ...  zj76 how  \\\n",
              "text                                                ...             \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.   ...         0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...  ...         0   \n",
              "@SouthwestAir I would.                              ...         0   \n",
              "@USAirways trying to Cancelled Flight a flight ...  ...         0   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...  ...         0   \n",
              "\n",
              "                                                    zj76 how did  zone  \\\n",
              "text                                                                     \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.              0     0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...             0     0   \n",
              "@SouthwestAir I would.                                         0     0   \n",
              "@USAirways trying to Cancelled Flight a flight ...             0     0   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...             0     0   \n",
              "\n",
              "                                                    zone was  zone was after  \\\n",
              "text                                                                           \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.          0               0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...         0               0   \n",
              "@SouthwestAir I would.                                     0               0   \n",
              "@USAirways trying to Cancelled Flight a flight ...         0               0   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...         0               0   \n",
              "\n",
              "                                                    zsdgzydnde  zukes  \\\n",
              "text                                                                    \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.            0      0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...           0      0   \n",
              "@SouthwestAir I would.                                       0      0   \n",
              "@USAirways trying to Cancelled Flight a flight ...           0      0   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...           0      0   \n",
              "\n",
              "                                                    zukes non  \\\n",
              "text                                                            \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.           0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...          0   \n",
              "@SouthwestAir I would.                                      0   \n",
              "@USAirways trying to Cancelled Flight a flight ...          0   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...          0   \n",
              "\n",
              "                                                    zukes non vegan  \\\n",
              "text                                                                  \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.                 0   \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...                0   \n",
              "@SouthwestAir I would.                                            0   \n",
              "@USAirways trying to Cancelled Flight a flight ...                0   \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...                0   \n",
              "\n",
              "                                                    zv2pt6trk9  \n",
              "text                                                            \n",
              "@JetBlue Maybe I'll just go to Cleveland instead.            0  \n",
              "smh RT @JetBlue: Our fleet's on fleek. http://t...           0  \n",
              "@SouthwestAir I would.                                       0  \n",
              "@USAirways trying to Cancelled Flight a flight ...           0  \n",
              "@AmericanAir you are beyond redemption. Jfk. Ba...           0  \n",
              "\n",
              "[5 rows x 70613 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b261b31e-3487-4da4-ae47-df91e9782a56\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>00</th>\n",
              "      <th>00 phone</th>\n",
              "      <th>00 phone hold</th>\n",
              "      <th>00 pm</th>\n",
              "      <th>00 pm that</th>\n",
              "      <th>000</th>\n",
              "      <th>000 air</th>\n",
              "      <th>000 air miles</th>\n",
              "      <th>000 crewmembers</th>\n",
              "      <th>000 crewmembers embody</th>\n",
              "      <th>...</th>\n",
              "      <th>zj76 how</th>\n",
              "      <th>zj76 how did</th>\n",
              "      <th>zone</th>\n",
              "      <th>zone was</th>\n",
              "      <th>zone was after</th>\n",
              "      <th>zsdgzydnde</th>\n",
              "      <th>zukes</th>\n",
              "      <th>zukes non</th>\n",
              "      <th>zukes non vegan</th>\n",
              "      <th>zv2pt6trk9</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>@JetBlue Maybe I'll just go to Cleveland instead.</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smh RT @JetBlue: Our fleet's on fleek. http://t.co/IRiXaIfJJX</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>@SouthwestAir I would.</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>@USAirways trying to Cancelled Flight a flight urgently...get hung up on twice??? Sweet refund policy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>@AmericanAir you are beyond redemption. Jfk. Baggage claim looks like a luggage warehouse</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 70613 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b261b31e-3487-4da4-ae47-df91e9782a56')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b261b31e-3487-4da4-ae47-df91e9782a56 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b261b31e-3487-4da4-ae47-df91e9782a56');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(max_features=10)\n",
        "cv.fit(train['text'])\n",
        "cv.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LymSPTAHuYlg",
        "outputId": "0da39b57-055f-4af2-ade2-5ec0471df649"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['and', 'flight', 'for', 'jetblue', 'on', 'southwestair', 'the',\n",
              "       'to', 'united', 'you'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get rid of em stop words"
      ],
      "metadata": {
        "id": "G1BuZwK51QE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(stop_words='english',max_features=10)\n",
        "cv.fit(train[\"text\"])\n",
        "cv.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd95ClrT1OhL",
        "outputId": "8bcf599d-93a5-44b9-9585-b46838f263c8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['americanair', 'flight', 'http', 'jetblue', 'service',\n",
              "       'southwestair', 'thank', 'thanks', 'united', 'usairways'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we see http,that is redundant , maybe the website we can scrape but it requires manual processing. Also we can futher use regex to remove:\n",
        "- urls\n",
        "- look for exclamation marks and other punctuation marks\n",
        "- emojis in text :) or :(\n",
        "- capitalization ratio etc\n"
      ],
      "metadata": {
        "id": "_2LYoczE1hPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Downside is that if a feature/token/word is not in the corpus of traning text , if it occurs in the test set, it would completely disregard it .\n",
        "- Upside is that we get intepretable features with combination with a tree based model we get interpretable model, which can futher the feature selection and extraction process.\n"
      ],
      "metadata": {
        "id": "In6hQBlK2hhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we bout to do?\n",
        "\n",
        "1. Take in a pipeline that has both the feature engineering pipeline and the\n",
        "model in it.\n",
        "2. Run a cross-validated grid search on the pipeline as a whole, tuning parameters\n",
        "for the model and the feature engineering algorithms at the same time. This is\n",
        "run on the training set.\n",
        "3. Pick the set of parameters that maximizes accuracy.\n",
        "4. Print a classification report on the test set.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Emhb2xiT4oDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the CountVectorizer‚Äôs features in our ML pipeline"
      ],
      "metadata": {
        "id": "8zaaLwAR9Rw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=3333)\n",
        "\n",
        "ml_pipeline = Pipeline([\n",
        "    (\"vectorizer\",CountVectorizer()),\n",
        "    (\"classifier\",clf)\n",
        "])\n",
        "\n",
        "params = {\n",
        "    \"vectorizer__lowercase\":[True,False],\n",
        "    \"vectorizer__stop_words\":[None,\"english\"],\n",
        "    \"vectorizer__max_features\":[100, 1000, 5000],\n",
        "    \"vectorizer__ngram_range\":[(1,1),(1,3)],\n",
        "    \"classifier__C\":[1e-1,1e0,1e1]\n",
        "    }\n",
        "print(\"Count Vectorizer + Logistic Regression ------------------------------------------------------------\")\n",
        "best_model, y_preds = advanced_grid_search(\n",
        "    train[\"text\"],train[\"sentiment\"],test[\"text\"],test[\"sentiment\"],\n",
        "    ml_pipeline, params\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7URAuEr42jU",
        "outputId": "75922aee-6bfa-4df1-910a-b0806f459fdd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count Vectorizer + Logistic Regression ------------------------------------------------------------\n",
            "Training the model121.91 seconds\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.79      0.77      0.78       243\n",
            "     neutral       0.75      0.78      0.77       260\n",
            "    positive       0.85      0.83      0.84       269\n",
            "\n",
            "    accuracy                           0.80       772\n",
            "   macro avg       0.80      0.79      0.79       772\n",
            "weighted avg       0.80      0.80      0.80       772\n",
            "\n",
            "Best params : {'classifier__C': 1.0, 'vectorizer__lowercase': True, 'vectorizer__max_features': 5000, 'vectorizer__ngram_range': (1, 1), 'vectorizer__stop_words': None}\n",
            "Overall took122.05 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF vectorization"
      ],
      "metadata": {
        "id": "90wzJtkxriaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tf-id = $ TF(t,d) √ó IDF(t) $  where $ IDF(t) = Log [ (1+n)/ 1+df(t)] + 1 $ where  $ df(t) $ is total number of times the term $t$ occurs in the whole document"
      ],
      "metadata": {
        "id": "n2MxOVtvAM6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another Bag of words vectorizer essentially"
      ],
      "metadata": {
        "id": "JRRdwod6Cyh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfid_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "\n",
        "tfid_vectorizer.fit(train[\"text\"])\n",
        "idf = pd.DataFrame({\"feature_name\":tfid_vectorizer.get_feature_names_out(), 'idf_weights':tfid_vectorizer.idf_})"
      ],
      "metadata": {
        "id": "8FyRofVVB_dS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf.sort_values(\"idf_weights\", ascending=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "CwDLTcYSC9iw",
        "outputId": "9c7c223f-65f1-4e68-dea8-7c8793b3d3b3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     feature_name  idf_weights\n",
              "913        united     2.497463\n",
              "347        flight     2.558630\n",
              "821  southwestair     2.586713\n",
              "495       jetblue     2.648723\n",
              "66    americanair     2.695243\n",
              "..            ...          ...\n",
              "532          lies     7.649308\n",
              "531       license     7.649308\n",
              "388          girl     7.649308\n",
              "485          ipad     7.649308\n",
              "484           ios     7.649308\n",
              "\n",
              "[1000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-35645cfb-ce3e-4ff2-b400-0c889ce33f42\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_name</th>\n",
              "      <th>idf_weights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>913</th>\n",
              "      <td>united</td>\n",
              "      <td>2.497463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>flight</td>\n",
              "      <td>2.558630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>821</th>\n",
              "      <td>southwestair</td>\n",
              "      <td>2.586713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>jetblue</td>\n",
              "      <td>2.648723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>americanair</td>\n",
              "      <td>2.695243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532</th>\n",
              "      <td>lies</td>\n",
              "      <td>7.649308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>531</th>\n",
              "      <td>license</td>\n",
              "      <td>7.649308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>girl</td>\n",
              "      <td>7.649308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>485</th>\n",
              "      <td>ipad</td>\n",
              "      <td>7.649308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>484</th>\n",
              "      <td>ios</td>\n",
              "      <td>7.649308</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows √ó 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35645cfb-ce3e-4ff2-b400-0c889ce33f42')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-35645cfb-ce3e-4ff2-b400-0c889ce33f42 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-35645cfb-ce3e-4ff2-b400-0c889ce33f42');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"idf\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"feature_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"bna\",\n          \"americanairlines\",\n          \"imagine\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"idf_weights\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8229887211367178,\n        \"min\": 2.4974628691509118,\n        \"max\": 7.649308331798691,\n        \"num_unique_values\": 91,\n        \"samples\": [\n          4.992551425084032,\n          4.45063521424801,\n          5.346723238804645\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use this in our ML pipeline"
      ],
      "metadata": {
        "id": "BJMxa3ZyDfi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_pipeline = Pipeline([\n",
        "    ('tfid_vectorizer',TfidfVectorizer()),\n",
        "    (\"classifier\",clf)\n",
        "])\n",
        "params = {\n",
        "    \"tfid_vectorizer__lowercase\":[True,False],\n",
        "    \"tfid_vectorizer__stop_words\":[None,\"english\"],\n",
        "    \"tfid_vectorizer__max_features\":[100, 1000, 5000],\n",
        "    \"tfid_vectorizer__ngram_range\":[(1,1),(1,3)],\n",
        "    \"classifier__C\":[1e-1,1e0,1e1]\n",
        "    }"
      ],
      "metadata": {
        "id": "rqMcgWOJDh__"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TF-IDF Vectorizer + Log Reg\\n=====================\")\n",
        "advanced_grid_search(train['text'], train['sentiment'], test['text'], test['sentiment'],ml_pipeline, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkqqkd7RDwfk",
        "outputId": "b63e4d88-5290-413b-f8eb-3c15bf0cedcb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vectorizer + Log Reg\n",
            "=====================\n",
            "Training the model100.10 seconds\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.80      0.84      0.82       243\n",
            "     neutral       0.83      0.80      0.81       260\n",
            "    positive       0.88      0.87      0.88       269\n",
            "\n",
            "    accuracy                           0.84       772\n",
            "   macro avg       0.84      0.84      0.84       772\n",
            "weighted avg       0.84      0.84      0.84       772\n",
            "\n",
            "Best params : {'classifier__C': 1.0, 'tfid_vectorizer__lowercase': True, 'tfid_vectorizer__max_features': 5000, 'tfid_vectorizer__ngram_range': (1, 3), 'tfid_vectorizer__stop_words': None}\n",
            "Overall took100.16 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Pipeline(steps=[('tfid_vectorizer',\n",
              "                  TfidfVectorizer(max_features=5000, ngram_range=(1, 3))),\n",
              "                 ('classifier', LogisticRegression(max_iter=3333))]),\n",
              " array(['negative', 'neutral', 'positive', 'neutral', 'neutral',\n",
              "        'positive', 'neutral', 'negative', 'positive', 'negative',\n",
              "        'neutral', 'negative', 'negative', 'positive', 'neutral',\n",
              "        'negative', 'negative', 'negative', 'positive', 'negative',\n",
              "        'positive', 'positive', 'positive', 'negative', 'neutral',\n",
              "        'negative', 'neutral', 'neutral', 'positive', 'neutral',\n",
              "        'negative', 'neutral', 'neutral', 'positive', 'positive',\n",
              "        'negative', 'neutral', 'positive', 'neutral', 'positive',\n",
              "        'positive', 'neutral', 'neutral', 'negative', 'neutral',\n",
              "        'negative', 'negative', 'positive', 'negative', 'positive',\n",
              "        'negative', 'negative', 'negative', 'positive', 'positive',\n",
              "        'positive', 'positive', 'neutral', 'positive', 'positive',\n",
              "        'positive', 'negative', 'neutral', 'positive', 'negative',\n",
              "        'negative', 'positive', 'positive', 'negative', 'negative',\n",
              "        'neutral', 'positive', 'positive', 'negative', 'negative',\n",
              "        'negative', 'neutral', 'positive', 'neutral', 'positive',\n",
              "        'negative', 'positive', 'neutral', 'negative', 'negative',\n",
              "        'positive', 'positive', 'neutral', 'negative', 'positive',\n",
              "        'positive', 'negative', 'negative', 'neutral', 'positive',\n",
              "        'positive', 'negative', 'positive', 'neutral', 'negative',\n",
              "        'negative', 'positive', 'negative', 'positive', 'negative',\n",
              "        'neutral', 'neutral', 'negative', 'negative', 'positive',\n",
              "        'neutral', 'negative', 'positive', 'positive', 'positive',\n",
              "        'neutral', 'negative', 'negative', 'positive', 'neutral',\n",
              "        'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral',\n",
              "        'negative', 'positive', 'negative', 'neutral', 'positive',\n",
              "        'negative', 'negative', 'negative', 'negative', 'neutral',\n",
              "        'negative', 'neutral', 'negative', 'negative', 'neutral',\n",
              "        'positive', 'positive', 'positive', 'neutral', 'positive',\n",
              "        'positive', 'neutral', 'neutral', 'negative', 'negative',\n",
              "        'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral',\n",
              "        'positive', 'neutral', 'positive', 'negative', 'neutral',\n",
              "        'neutral', 'negative', 'negative', 'positive', 'positive',\n",
              "        'positive', 'negative', 'neutral', 'neutral', 'positive',\n",
              "        'neutral', 'neutral', 'positive', 'negative', 'positive',\n",
              "        'positive', 'positive', 'neutral', 'neutral', 'neutral',\n",
              "        'positive', 'negative', 'positive', 'negative', 'positive',\n",
              "        'negative', 'positive', 'positive', 'neutral', 'neutral',\n",
              "        'neutral', 'positive', 'positive', 'negative', 'negative',\n",
              "        'positive', 'positive', 'positive', 'neutral', 'negative',\n",
              "        'neutral', 'positive', 'negative', 'neutral', 'neutral', 'neutral',\n",
              "        'neutral', 'positive', 'negative', 'negative', 'negative',\n",
              "        'positive', 'neutral', 'neutral', 'positive', 'positive',\n",
              "        'positive', 'negative', 'positive', 'neutral', 'neutral',\n",
              "        'neutral', 'neutral', 'neutral', 'negative', 'positive',\n",
              "        'negative', 'neutral', 'neutral', 'positive', 'neutral',\n",
              "        'negative', 'positive', 'negative', 'negative', 'positive',\n",
              "        'neutral', 'positive', 'positive', 'neutral', 'neutral',\n",
              "        'negative', 'neutral', 'positive', 'positive', 'positive',\n",
              "        'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral',\n",
              "        'negative', 'negative', 'negative', 'negative', 'neutral',\n",
              "        'negative', 'neutral', 'negative', 'neutral', 'neutral', 'neutral',\n",
              "        'neutral', 'positive', 'negative', 'positive', 'positive',\n",
              "        'negative', 'neutral', 'negative', 'neutral', 'negative',\n",
              "        'neutral', 'negative', 'positive', 'negative', 'neutral',\n",
              "        'neutral', 'positive', 'neutral', 'positive', 'negative',\n",
              "        'neutral', 'negative', 'negative', 'neutral', 'neutral', 'neutral',\n",
              "        'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'positive',\n",
              "        'negative', 'positive', 'neutral', 'positive', 'neutral',\n",
              "        'negative', 'neutral', 'positive', 'negative', 'positive',\n",
              "        'negative', 'positive', 'negative', 'neutral', 'negative',\n",
              "        'negative', 'neutral', 'positive', 'neutral', 'neutral',\n",
              "        'negative', 'neutral', 'positive', 'negative', 'neutral',\n",
              "        'positive', 'neutral', 'negative', 'neutral', 'negative',\n",
              "        'neutral', 'negative', 'neutral', 'negative', 'positive',\n",
              "        'positive', 'negative', 'neutral', 'neutral', 'positive',\n",
              "        'positive', 'neutral', 'negative', 'positive', 'positive',\n",
              "        'positive', 'neutral', 'positive', 'positive', 'negative',\n",
              "        'negative', 'positive', 'negative', 'negative', 'positive',\n",
              "        'neutral', 'neutral', 'negative', 'negative', 'negative',\n",
              "        'positive', 'positive', 'negative', 'positive', 'neutral',\n",
              "        'negative', 'positive', 'negative', 'neutral', 'neutral',\n",
              "        'neutral', 'neutral', 'positive', 'positive', 'neutral',\n",
              "        'positive', 'positive', 'negative', 'neutral', 'negative',\n",
              "        'negative', 'negative', 'negative', 'neutral', 'neutral',\n",
              "        'positive', 'neutral', 'positive', 'neutral', 'positive',\n",
              "        'negative', 'positive', 'positive', 'negative', 'positive',\n",
              "        'positive', 'positive', 'positive', 'neutral', 'positive',\n",
              "        'positive', 'positive', 'negative', 'positive', 'positive',\n",
              "        'negative', 'neutral', 'positive', 'negative', 'neutral',\n",
              "        'negative', 'positive', 'neutral', 'neutral', 'neutral',\n",
              "        'negative', 'negative', 'neutral', 'negative', 'negative',\n",
              "        'positive', 'neutral', 'neutral', 'positive', 'negative',\n",
              "        'positive', 'neutral', 'positive', 'neutral', 'positive',\n",
              "        'positive', 'positive', 'positive', 'neutral', 'neutral',\n",
              "        'negative', 'positive', 'positive', 'positive', 'negative',\n",
              "        'positive', 'negative', 'negative', 'positive', 'positive',\n",
              "        'positive', 'neutral', 'neutral', 'positive', 'neutral',\n",
              "        'negative', 'positive', 'positive', 'negative', 'neutral',\n",
              "        'negative', 'positive', 'negative', 'negative', 'neutral',\n",
              "        'neutral', 'neutral', 'positive', 'neutral', 'negative',\n",
              "        'positive', 'negative', 'neutral', 'negative', 'negative',\n",
              "        'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'neutral',\n",
              "        'neutral', 'negative', 'neutral', 'positive', 'neutral',\n",
              "        'positive', 'positive', 'positive', 'neutral', 'neutral',\n",
              "        'positive', 'neutral', 'negative', 'negative', 'positive',\n",
              "        'positive', 'positive', 'positive', 'neutral', 'neutral',\n",
              "        'negative', 'neutral', 'neutral', 'positive', 'negative',\n",
              "        'positive', 'positive', 'negative', 'negative', 'positive',\n",
              "        'neutral', 'positive', 'negative', 'negative', 'negative',\n",
              "        'neutral', 'positive', 'negative', 'negative', 'negative',\n",
              "        'positive', 'negative', 'negative', 'positive', 'positive',\n",
              "        'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'negative',\n",
              "        'neutral', 'positive', 'positive', 'negative', 'neutral',\n",
              "        'positive', 'neutral', 'negative', 'negative', 'negative',\n",
              "        'negative', 'negative', 'positive', 'negative', 'negative',\n",
              "        'negative', 'negative', 'positive', 'neutral', 'negative',\n",
              "        'positive', 'neutral', 'positive', 'positive', 'positive',\n",
              "        'positive', 'neutral', 'negative', 'negative', 'positive',\n",
              "        'positive', 'positive', 'neutral', 'neutral', 'neutral',\n",
              "        'negative', 'negative', 'positive', 'positive', 'neutral',\n",
              "        'positive', 'negative', 'negative', 'positive', 'negative',\n",
              "        'positive', 'neutral', 'negative', 'positive', 'neutral',\n",
              "        'neutral', 'positive', 'negative', 'neutral', 'positive',\n",
              "        'neutral', 'neutral', 'negative', 'neutral', 'positive',\n",
              "        'negative', 'negative', 'neutral', 'positive', 'negative',\n",
              "        'positive', 'neutral', 'negative', 'positive', 'positive',\n",
              "        'negative', 'positive', 'positive', 'positive', 'positive',\n",
              "        'positive', 'neutral', 'neutral', 'negative', 'negative',\n",
              "        'negative', 'negative', 'positive', 'negative', 'positive',\n",
              "        'negative', 'negative', 'neutral', 'negative', 'negative',\n",
              "        'negative', 'positive', 'negative', 'neutral', 'positive',\n",
              "        'positive', 'positive', 'positive', 'positive', 'neutral',\n",
              "        'neutral', 'negative', 'neutral', 'positive', 'neutral',\n",
              "        'negative', 'positive', 'negative', 'neutral', 'positive',\n",
              "        'positive', 'positive', 'neutral', 'positive', 'negative',\n",
              "        'neutral', 'positive', 'negative', 'positive', 'positive',\n",
              "        'negative', 'neutral', 'positive', 'negative', 'negative',\n",
              "        'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'negative',\n",
              "        'neutral', 'positive', 'neutral', 'negative', 'negative',\n",
              "        'neutral', 'negative', 'negative', 'neutral', 'negative',\n",
              "        'negative', 'negative', 'positive', 'negative', 'neutral',\n",
              "        'positive', 'negative', 'positive', 'neutral', 'negative',\n",
              "        'positive', 'negative', 'positive', 'neutral', 'positive',\n",
              "        'neutral', 'neutral', 'positive', 'neutral', 'negative', 'neutral',\n",
              "        'negative', 'negative', 'neutral', 'negative', 'positive',\n",
              "        'neutral', 'positive', 'positive', 'negative', 'negative',\n",
              "        'positive', 'neutral', 'positive', 'neutral', 'negative',\n",
              "        'negative', 'positive', 'positive', 'neutral', 'neutral',\n",
              "        'positive', 'negative', 'negative', 'neutral', 'negative',\n",
              "        'positive', 'negative', 'negative', 'positive', 'neutral',\n",
              "        'negative', 'negative', 'neutral', 'neutral', 'negative',\n",
              "        'positive', 'negative', 'neutral', 'positive', 'negative',\n",
              "        'neutral', 'negative', 'neutral', 'negative', 'positive',\n",
              "        'negative', 'positive', 'neutral', 'neutral', 'positive',\n",
              "        'neutral', 'positive', 'positive', 'positive', 'negative',\n",
              "        'negative', 'positive', 'positive', 'neutral', 'negative',\n",
              "        'negative', 'neutral', 'neutral', 'neutral', 'positive', 'neutral',\n",
              "        'negative', 'positive', 'negative', 'positive', 'negative',\n",
              "        'positive', 'neutral', 'positive', 'positive', 'negative',\n",
              "        'neutral', 'positive', 'negative', 'negative', 'neutral',\n",
              "        'negative', 'positive', 'neutral', 'neutral', 'neutral'],\n",
              "       dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like normalizing token counts to extract originality in tokens\n",
        "helps our model understand sentiment a bit better"
      ],
      "metadata": {
        "id": "JE3CoIOhEZ5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature improvement"
      ],
      "metadata": {
        "id": "oQNyEodYrME0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting Max features is almost enough, but we want to extract more features too.\n",
        "- Remove hashtags, but note down the hash tag value first, then try another text processing(tfid,count, hashing) for the new columm\n",
        "- remove urls, but see what can be extracted, like the website name etc\n",
        "- remove @ sign but add to another column named mentions and then use count vectorizer\n",
        "- remove numbers as we can't use LLMs etc to see the context of the numbers\n",
        "- remove emojis but note it down in a column"
      ],
      "metadata": {
        "id": "_nrFsGfAEzbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning noise from text"
      ],
      "metadata": {
        "id": "XTTGcTXirngx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's clean all the noise but extract relevan information about it"
      ],
      "metadata": {
        "id": "B4Kc8ulWHS97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rough Work"
      ],
      "metadata": {
        "id": "RVoONyBTHi9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "LAqAl_nrHXq1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_pattern = re.compile(r'http?://(?:www\\.)?([^/\\s]+)\\S*')"
      ],
      "metadata": {
        "id": "3wth-8MvHtsE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[\"urls\"] = train[\"text\"].apply(lambda x: \" \".join(url_pattern.findall(x)))\n",
        "train.loc[train[\"urls\"].apply(lambda x: len(x)>0)][\"urls\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "DVToBV0CHtp2",
        "outputId": "affcfa9f-d527-4098-8a72-c74fe8f82fc4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "urls\n",
              "t.co              273\n",
              "t.co t.co           6\n",
              "t.co t.co t.co      1\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>urls</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>t.co</th>\n",
              "      <td>273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>t.co t.co</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>t.co t.co t.co</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url_pattern = re.compile(r'http?://(?:www\\.)?([^/\\s]+)\\S*')\n",
        "hashtag_pattern = re.compile(r'#(\\w+)')\n",
        "train[\"hashtags\"] = train[\"text\"].apply(lambda x: \" \".join(hashtag_pattern.findall(x)))\n",
        "train.loc[train[\"hashtags\"].apply(lambda x: len(x.split())>1)][\"hashtags\"].value_counts() #more thatn 1 hastags\n",
        "train.loc[train[\"hashtags\"].apply(lambda x: len(x)>1)][\"hashtags\"].value_counts() #atleast one hashtags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "JRZg6eiBIbK0",
        "outputId": "793b32d5-7f16-412d-bba2-183c6c3fbb0e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "hashtags\n",
              "DestinationDragons    26\n",
              "customerservice        6\n",
              "UnitedAirlines         6\n",
              "fail                   4\n",
              "disappointed           4\n",
              "                      ..\n",
              "thanks                 1\n",
              "bna                    1\n",
              "1786                   1\n",
              "SWfan                  1\n",
              "be                     1\n",
              "Name: count, Length: 447, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hashtags</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>DestinationDragons</th>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>customerservice</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>UnitedAirlines</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fail</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappointed</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thanks</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bna</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1786</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SWfan</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>be</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>447 rows √ó 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url_pattern = re.compile(r'http?://(?:www\\.)?([^/\\s]+)\\S*')\n",
        "hashtag_pattern = re.compile(r'#(\\w+)')\n",
        "mention_pattern = re.compile(r'@(\\w+)')\n",
        "train[\"mention\"] = train[\"text\"].apply(lambda x: \" \".join(mention_pattern .findall(x)))\n",
        "train.loc[train[\"mention\"].apply(lambda x: len(x)>0)][\"mention\"].value_counts()\n",
        "train.loc[train[\"mention\"].apply(lambda x: len(x.split())>1)][\"mention\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "s7ws6XbjHte2",
        "outputId": "2fa6f18c-daa3-43b8-8adc-69e3812c3469"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "mention\n",
              "USAirways AmericanAir                                              23\n",
              "SouthwestAir FortuneMagazine                                        8\n",
              "SouthwestAir Imaginedragons                                         7\n",
              "AmericanAir dfwairport                                              4\n",
              "JetBlue WSJ                                                         4\n",
              "                                                                   ..\n",
              "VirginAmerica ladygaga carrieunderwood ladygaga carrieunderwood     1\n",
              "united HeathrowAirport                                              1\n",
              "SouthwestAir poisonpill76                                           1\n",
              "AmericanAir cityandsand                                             1\n",
              "VirginAmerica KCIAirport                                            1\n",
              "Name: count, Length: 302, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mention</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>USAirways AmericanAir</th>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SouthwestAir FortuneMagazine</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SouthwestAir Imaginedragons</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AmericanAir dfwairport</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>JetBlue WSJ</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VirginAmerica ladygaga carrieunderwood ladygaga carrieunderwood</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>united HeathrowAirport</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SouthwestAir poisonpill76</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AmericanAir cityandsand</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VirginAmerica KCIAirport</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>302 rows √ó 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url_pattern = re.compile(r'http?://(?:www\\.)?([^/\\s]+)\\S*')\n",
        "hashtag_pattern = re.compile(r'#(\\w+)')\n",
        "mention_pattern = re.compile(r'@(\\w+)')\n",
        "emoji_pattern = re.compile(r'[^\\x00-\\x7F]+')\n",
        "train[\"emojis\"] = train[\"text\"].apply(lambda x: \" \".join(emoji_pattern .findall(x)))\n",
        "#train.loc[train[\"emojis\"].apply(lambda x: len(x)>0)][\"emojis\"].value_counts()\n",
        "#train.loc[train[\"emojis\"].apply(lambda x: len(x.split())>1)][\"emojis\"].value_counts()\n"
      ],
      "metadata": {
        "id": "VwrJP6QDKDym"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_pattern = re.compile(r'\\d+')\n",
        "train[\"numbers\"] = train[\"text\"].apply(lambda x: \" \".join(number_pattern .findall(x)))\n",
        "train.loc[train[\"numbers\"].apply(lambda x: len(x)>0)][\"numbers\"].value_counts()\n",
        "train.loc[train[\"numbers\"].apply(lambda x: len(x.split())>1)][[\"numbers\",\"text\"]]#.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "rTAQYlc8KUaB",
        "outputId": "7d9bd00e-ed4d-42ad-e0e3-f97895969e46"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  numbers                                               text\n",
              "1030  10 9 17 20 15 7 8 7  @united The guidelines say 10x9x17, my bag is ...\n",
              "1933               5 1 15  ‚Äú@JetBlue: Our fleet's on fleek. http://t.co/X...\n",
              "2648                2 4 2  @USAirways is okay for u 2 Cancelled Flight ch...\n",
              "1097                9 180  @SouthwestAir has the smoooothest flight atten...\n",
              "3800            3231 4 45  @AmericanAir 3231DTW to LAG at 4:45. Flight Ca...\n",
              "...                   ...                                                ...\n",
              "2255                0 0 8  @JetBlue and The from @WSJ Team to Offer In-#F...\n",
              "2441                  2 3  @JetBlue our #FoodAllergy community. IF you wa...\n",
              "2259                  6 8  @JetBlue flight booked! Heading out to Califor...\n",
              "680                10 1 2  @united - sitting in seat 10D on a flight back...\n",
              "1860                 5 68  ‚Äú@JetBlue: Our fleet's on fleek. http://t.co/b...\n",
              "\n",
              "[489 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74a4ce72-5638-46d4-a3e8-bbb31fca731e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>numbers</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1030</th>\n",
              "      <td>10 9 17 20 15 7 8 7</td>\n",
              "      <td>@united The guidelines say 10x9x17, my bag is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1933</th>\n",
              "      <td>5 1 15</td>\n",
              "      <td>‚Äú@JetBlue: Our fleet's on fleek. http://t.co/X...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2648</th>\n",
              "      <td>2 4 2</td>\n",
              "      <td>@USAirways is okay for u 2 Cancelled Flight ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1097</th>\n",
              "      <td>9 180</td>\n",
              "      <td>@SouthwestAir has the smoooothest flight atten...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3800</th>\n",
              "      <td>3231 4 45</td>\n",
              "      <td>@AmericanAir 3231DTW to LAG at 4:45. Flight Ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2255</th>\n",
              "      <td>0 0 8</td>\n",
              "      <td>@JetBlue and The from @WSJ Team to Offer In-#F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2441</th>\n",
              "      <td>2 3</td>\n",
              "      <td>@JetBlue our #FoodAllergy community. IF you wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2259</th>\n",
              "      <td>6 8</td>\n",
              "      <td>@JetBlue flight booked! Heading out to Califor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>680</th>\n",
              "      <td>10 1 2</td>\n",
              "      <td>@united - sitting in seat 10D on a flight back...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1860</th>\n",
              "      <td>5 68</td>\n",
              "      <td>‚Äú@JetBlue: Our fleet's on fleek. http://t.co/b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>489 rows √ó 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74a4ce72-5638-46d4-a3e8-bbb31fca731e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-74a4ce72-5638-46d4-a3e8-bbb31fca731e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-74a4ce72-5638-46d4-a3e8-bbb31fca731e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 489,\n  \"fields\": [\n    {\n      \"column\": \"numbers\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 425,\n        \"samples\": [\n          \"4 48 3\",\n          \"3 360\",\n          \"30 5612\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 489,\n        \"samples\": [\n          \"@SouthwestAir you need to get your act together. You new this morning at 830 our plane was malfunctioning. Yet I've been delayed 3 times ..\",\n          \"@AmericanAir Flight 1679 (N76200) prepares for flight at @FlyTPA before departing for @fly2ohare http://t.co/XbKvcraOKn\",\n          \"@AmericanAir Chicago seen from seat 6A, AA 1620. So far a great ride! On to PDX! http://t.co/X4rsvAGIjN\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sCEHvgsgbh3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complete Pipeline"
      ],
      "metadata": {
        "id": "yp8wN2qOHnJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "class TextFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.number_pattern = re.compile(r'\\d+')\n",
        "    self.url_pattern = re.compile(r'http?://(?:www\\.)?([^/\\s]+)\\S*')\n",
        "    self.hashtag_pattern = re.compile(r'#(\\w+)')\n",
        "    self.mention_pattern = re.compile(r'@(\\w+)')\n",
        "    self.emoji_pattern = re.compile(r'[^\\x00-\\x7F]+')\n",
        "\n",
        "\n",
        "  def fit(self, C, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    s = pd.Series(X)\n",
        "    features = pd.DataFrame()\n",
        "\n",
        "    features[\"hashtags\"] = s.apply(lambda x:\" \".join(self.hashtag_pattern.findall(x)))\n",
        "    features[\"urls\"] = s.apply(lambda x:\" \".join(self.url_pattern.findall(x)))\n",
        "    features[\"mentions\"] = s.apply(lambda x:\" \".join(self.mention_pattern.findall(x)))\n",
        "    features[\"excl_count\"] = s.apply(lambda x:x.count(\"!\"))\n",
        "    features[\"ques_count\"] = s.apply(lambda x:x.count(\"?\"))\n",
        "    features[\"comma_fullstop_count\"] = s.apply(lambda x:x.count(\".\") + x.count(\",\"))\n",
        "    features[\"emojis\"] = s.apply(lambda x:\" \".join(self.emoji_pattern.findall(x)))\n",
        "\n",
        "    cleaned_text = s.str.replace(self.url_pattern, '', regex=True) \\\n",
        "                      .str.replace(self.hashtag_pattern, '', regex=True) \\\n",
        "                      .str.replace(self.mention_pattern, '', regex=True) \\\n",
        "                      .str.replace(self.number_pattern, '', regex=True) \\\n",
        "                      .str.replace(self.emoji_pattern, '', regex=True) \\\n",
        "                      .str.strip()\n",
        "    features[\"cleaned_text\"] = cleaned_text\n",
        "    return features\n",
        "def get_col(df, col_name):\n",
        "    return df[col_name]"
      ],
      "metadata": {
        "id": "EP2y71SrHXoR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Needed to split certain columns, with 2 emojis or 2 hashtags or 2 mentions etc"
      ],
      "metadata": {
        "id": "TI9NO46rbi-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extractor = TextFeatureExtractor()\n",
        "processor = ColumnTransformer([\n",
        "    ('text_tfidf', TfidfVectorizer(max_features=1000), 'cleaned_text'),\n",
        "    ('hash_vec', CountVectorizer(), 'hashtags'),\n",
        "    ('mention_vec', CountVectorizer(), 'mentions'),\n",
        "    ('url_vec', CountVectorizer(), 'urls'),\n",
        "    ('emoji_vec', CountVectorizer(token_pattern=r\"\\S\"), 'emojis'),\n",
        "    ('excl_scalar', 'passthrough', ['excl_count']),\n",
        "    ('ques_scalar', 'passthrough', ['ques_count']),\n",
        "    ('comma_fullstop_scalar', 'passthrough', ['comma_fullstop_count'])\n",
        "])\n",
        "pipeline = Pipeline([\n",
        "    ('extract', extractor),\n",
        "    ('vectorize', processor),\n",
        "    ('dim_reduction', TruncatedSVD(n_components=1000)),\n",
        "    (\"classifier\",clf)\n",
        "])"
      ],
      "metadata": {
        "id": "zSaf5-_2Q_Om"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_model = pipeline.fit(train['text'],train['sentiment'])\n",
        "y_preds= clf_model.predict(test['text'])\n",
        "print(classification_report(y_true=test['sentiment'], y_pred=y_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdAxhgjlW8UK",
        "outputId": "dd82962f-4918-4703-d159-6382be0b79bc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.78      0.77      0.78       243\n",
            "     neutral       0.79      0.80      0.79       260\n",
            "    positive       0.86      0.86      0.86       269\n",
            "\n",
            "    accuracy                           0.81       772\n",
            "   macro avg       0.81      0.81      0.81       772\n",
            "weighted avg       0.81      0.81      0.81       772\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not verygood, but not bad either.<br>\n",
        "Almost same metrics as the author but atleast 10 times faster in training"
      ],
      "metadata": {
        "id": "KleF-nBRZslt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature extraction\n",
        "\n",
        "just use truncated SVD bro lmao"
      ],
      "metadata": {
        "id": "i-jVfP2FrJfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "This notebook performs **feature engineering for sentiment classification** on a cleaned version of the well-known **Twitter US Airline Sentiment** dataset (`cleaned_airline_tweets.csv`). The task is multiclass classification: predict tweet sentiment (`positive`, `neutral`, `negative`) from the tweet `text`.  \n",
        "\n",
        "The workflow starts with basic text vectorization baselines (**CountVectorizer** + **TfidfVectorizer** + Logistic Regression), then moves to a more sophisticated **custom feature extraction pipeline** that:\n",
        "\n",
        "- extracts domain-relevant signals (hashtags, mentions, URLs, emojis, punctuation counts)\n",
        "- cleans the main text by removing those elements\n",
        "- applies different vectorizers to each extracted part\n",
        "- reduces dimensionality with **TruncatedSVD**\n",
        "- feeds everything into a classifier\n",
        "\n",
        "The focus is on showing how thoughtful **pre-processing + feature construction** (beyond plain bag-of-words / TF-IDF) can enrich the signal for social media sentiment tasks.\n",
        "\n",
        "### All Steps Involved + Why They Were Done\n",
        "\n",
        "1. **Data Loading & Quick EDA**  \n",
        "   - Read CSV, check columns, unique sentiments, class distribution, unique tweets.  \n",
        "   - Stratified train/test split (80/20) on sentiment label.  \n",
        "   **Why**: Understand imbalance (negative tweets usually dominate airline data), confirm text is the main input, ensure test set mirrors train distribution.\n",
        "\n",
        "2. **Simple Text Vectorization Baselines**  \n",
        "   - `CountVectorizer` experiments: single words, n-grams (1‚Äì3), stop words, max_features limit.  \n",
        "   - `TfidfVectorizer` with similar hyperparameters.  \n",
        "   - Wrapped in `Pipeline` + `LogisticRegression` ‚Üí grid search over: lowercase, stop_words, max_features, ngram_range, C.  \n",
        "   **Why**: Establish strong baseline using standard sparse text representations. Compare frequency counting vs. term importance weighting (TF-IDF usually wins slightly on sentiment tasks).\n",
        "\n",
        "3. **Feature Construction ‚Äì Noise Extraction & Cleaning**  \n",
        "   - Regex patterns to extract:  \n",
        "     ‚Äì URLs (domain part)  \n",
        "     ‚Äì Hashtags (#word)  \n",
        "     ‚Äì Mentions (@user)  \n",
        "     ‚Äì Emojis (non-ASCII)  \n",
        "     ‚Äì Numbers  \n",
        "   - Count punctuation: ! ? . ,  \n",
        "   - Clean main text by removing all extracted elements.  \n",
        "   **Why**: Social media contains strong sentiment signals in structure (lots of !, emojis, CAPS, mentions, hashtags). Removing them from main text prevents noise dilution, while keeping them as separate features preserves signal.\n",
        "\n",
        "4. **Custom Transformer** (`TextFeatureExtractor`)  \n",
        "   - Fits nothing (stateless), transforms Series of tweets ‚Üí DataFrame with:  \n",
        "     `cleaned_text`, `hashtags`, `urls`, `mentions`, `emojis`, `excl_count`, `ques_count`, `comma_fullstop_count`.  \n",
        "   **Why**: Encapsulate regex-based feature engineering in scikit-learn-compatible object ‚Üí reusable & pipeline-friendly.\n",
        "\n",
        "5. **Advanced Feature Processing Pipeline**  \n",
        "   - `ColumnTransformer` applies different vectorizers to different columns:  \n",
        "     ‚Äì `TfidfVectorizer(max_features=1000)` on `cleaned_text`  \n",
        "     ‚Äì `CountVectorizer` on `hashtags`, `mentions`, `urls`  \n",
        "     ‚Äì `CountVectorizer(token_pattern=r\"\\S\")` on `emojis` (treats each emoji as token)  \n",
        "     ‚Äì `passthrough` on scalar counts (!, ?, punctuation)  \n",
        "   - `TruncatedSVD(n_components=1000)` for dimensionality reduction.  \n",
        "   - Final classifier (LogisticRegression in example).  \n",
        "   **Why**: Heterogeneous feature types need specialized handling. SVD reduces curse of dimensionality after wide concatenation; helps especially with sparse multi-source vectors.\n",
        "\n",
        "6. **Model Training & Evaluation**  \n",
        "   - Fit pipeline end-to-end on train text ‚Üí predict on test text.  \n",
        "   - Print classification report.  \n",
        "   **Why**: Show realistic end-to-end performance of rich feature set vs. plain TF-IDF baseline. (Notebook notes similar metrics but much faster training.)\n",
        "\n",
        "7. **Comment on Feature Extraction**  \n",
        "   - Brief mention of TruncatedSVD as simple & effective way to compress high-dimensional text features.  \n",
        "   **Why**: Reminder that dimensionality reduction is often necessary after aggressive feature concatenation.\n",
        "\n",
        "### Packages / Modules / Techniques Used\n",
        "\n",
        "| Package / Module                          | Class / Function / Technique                              | Why It Was Used |\n",
        "|-------------------------------------------|------------------------------------------------------------|-----------------|\n",
        "| **pandas**                                | `read_csv`, `value_counts`, `apply`, `str.replace`, regex `.findall` | Data loading, grouping, custom regex-based feature extraction & cleaning |\n",
        "| **numpy**                                 | (implicit)                                                 | Array operations (not heavily used here) |\n",
        "| **matplotlib.pyplot**                     | (imported but not visibly used in code)                    | Potential for later plots (EDA) |\n",
        "| **time**                                  | `time.time()`                                              | Measure grid-search training duration |\n",
        "| **sklearn.model_selection**               | `train_test_split` (stratified), `GridSearchCV`            | Balanced split; hyperparameter tuning of vectorizer + model jointly |\n",
        "| **sklearn.linear_model**                  | `LogisticRegression`                                       | Fast, interpretable linear baseline classifier for text |\n",
        "| **sklearn.ensemble**                      | `ExtraTreesClassifier`, `RandomForestClassifier` (imported but not used in final pipeline) | Potential tree-based alternatives |\n",
        "| **sklearn.pipeline**                      | `Pipeline`                                                 | Chain feature extraction ‚Üí vectorization ‚Üí reduction ‚Üí classifier |\n",
        "| **sklearn.compose**                       | `ColumnTransformer`                                        | Apply different transformers to different output columns of custom extractor |\n",
        "| **sklearn.feature_extraction.text**       | `CountVectorizer`, `TfidfVectorizer`                       | Core sparse text vectorization (bag-of-words vs. tf-idf) |\n",
        "| **sklearn.preprocessing**                 | `FunctionTransformer` (imported but not heavily used)      | Potential wrapper for custom functions |\n",
        "| **sklearn.decomposition**                 | `TruncatedSVD`                                             | Linear dimensionality reduction on concatenated sparse matrix |\n",
        "| **sklearn.metrics**                       | `classification_report`                                    | Detailed per-class precision/recall/F1 evaluation |\n",
        "| **sklearn.base**                          | `BaseEstimator`, `TransformerMixin`                        | Create custom scikit-learn compatible `TextFeatureExtractor` |\n",
        "| **re**                                    | `compile`, `findall`                                       | Regex-based extraction of URLs, hashtags, mentions, emojis, numbers |\n",
        "| **ydata-profiling**                       | `ProfileReport` (commented out)                            | Automated EDA report (disabled for GitHub compatibility) |\n",
        "\n",
        "**Key Technique Highlights**  \n",
        "- **Custom `TextFeatureExtractor`** ‚Äî most important contribution: regex-driven multi-aspect feature construction tailored to Twitter/social media noise.  \n",
        "- **ColumnTransformer on heterogeneous text-derived columns** ‚Äî clean way to vectorize main cleaned text differently from metadata-like fields (hashtags, mentions, emojis, punctuation counts).  \n",
        "- Joint grid search over vectorizer hyperparameters + model regularisation ‚Äî realistic way to tune the full pipeline.  \n",
        "- Emphasis on cleaning while **preserving** structural sentiment cues rather than just removing everything.\n",
        "\n",
        "This notebook provides a practical demonstration of **social-media-specific text feature engineering** beyond vanilla Count/TF-IDF, showing how domain knowledge (Twitter syntax) can be turned into engineered columns that enrich a classical ML pipeline."
      ],
      "metadata": {
        "id": "l-tYhcf7dkyO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "45jPHeLWdnDk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}